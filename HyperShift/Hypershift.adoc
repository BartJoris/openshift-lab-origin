= Deploying Clusters using HyperShift

== Docs

* RHACM 2.5: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html/clusters/managing-your-clusters#hosted-control-plane-intro
* RHACM 2.6: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/multicluster_engine/multicluster_engine_overview#hosted-control-planes-intro

== Prerequisites

* Deploy an OpenShift 4.11 cluster
* On that cluster install Advanced Cluster Manager (operator and operand)
* Get your AWS credentials ready
** AWS Access Key
** AWS Secret Access Key
** Top level domain (sandboxXXXX.opentlc.com)
** Private and public key (e.g. ~/.ssh/${GUID}key.pem and ~/.ssh/${GUID}key.pub)
** OpenShift Pull Secret in a file `pullsecret.json`
* Set up AWS Credentials in `~/.aws/credentials`
+
[source]
----
[default]
aws_access_key_id = YOUR_AWS_ACCESS_KEY_ID
aws_secret_access_key = YOUR_AWS_SECRET_ACCESS_KEY
----

* Enable the HyperShift Preview Tech Preview
+
[source,sh]
----
oc patch multiclusterengine -n multicluster-engine --type=merge -p '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'
----

* Deploy Hypershift
** Create a file `import-hub.yaml`
+
[source,yaml]
----
---
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  labels:
    local-cluster: "true"
  name: local-cluster
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60
----

** Create the managed cluster
+
[source,sh]
----
oc apply -f import-hub.yaml
----

* Create an AWS S3 Bucket to store OICD Documents
+
[source,sh]
----
aws s3api create-bucket --bucket oidc-storage --region us-east-2 --create-bucket-configuration LocationConstraint=us-east-2
----

* Create a secret with AWS credential information in the hypershift namespace
+
[source,sh]
----
oc create secret generic hypershift-operator-oidc-provider-s3-credentials -n local-cluster --from-file=credentials=$HOME/.aws/credentials --from-literal=bucket=oidc-storage --from-literal=region=us-east-2

oc label secret hypershift-operator-oidc-provider-s3-credentials cluster.open-cluster-management.io/backup="" -n local-cluster
----

* Create Managed Cluster Addon File `managedclusteraddon.yaml`
+
[source,yaml]
----
---
apiVersion: addon.open-cluster-management.io/v1alpha1
kind: ManagedClusterAddOn
metadata:
  name: hypershift-addon
  namespace: local-cluster
spec:
  installNamespace: open-cluster-management-agent-addon
----

** Create the managed cluster addon
+
[source,sh]
----
oc apply -f managedclusteraddon.yaml
----

== Deploy a cluster using Hypershift

* Create namespace to collect resources
+
[source,sh]
----
oc create namespace hypershift
----

* Create AWS Credentials Secret for Hypershift
+
[source,sh]
----
oc create secret generic aws-cred -n hypershift --from-literal=baseDomain='sandboxXXXX.opentlc.com' --from-literal=aws_access_key_id='YOUR_ACCESS_KEY_ID' --from-literal=aws_secret_access_key='YOUR_SECRET_ACCESS_KEY' --from-file=pullSecret=~/.pullsecret.json --from-file=ssh-publickey=~/.ssh/wkacmkey.pub --from-file=ssh-privatekey=~/.ssh/wkacmkey.pem

oc label secret aws-cred -n hypershift cluster.open-cluster-management.io/backup=""
----

* Create YAML file `development.yaml` with cluster properties
+
[source,yaml]
----
---
apiVersion: cluster.open-cluster-management.io/v1alpha1
kind: HypershiftDeployment
metadata:
  name: development
  namespace: hypershift
spec:
  hostingCluster: local-cluster
  hostingNamespace: clusters
  hostedClusterSpec:
    networking:
      machineCIDR: 10.0.0.0/16    # Default
      networkType: OpenShiftSDN
      podCIDR: 10.132.0.0/14      # Default
      serviceCIDR: 172.31.0.0/16  # Default
    platform:
      type: AWS
    pullSecret:
      name: development-pull-secret    # This secret is created by the controller
    release:
      image: quay.io/openshift-release-dev/ocp-release:4.11.8-x86_64
    services:
    - service: APIServer
      servicePublishingStrategy:
        type: LoadBalancer
    - service: OAuthServer
      servicePublishingStrategy:
        type: Route
    - service: Konnectivity
      servicePublishingStrategy:
        type: Route
    - service: Ignition
      servicePublishingStrategy:
        type: Route
    sshKey: {}
  nodePools:
  - name: development
    spec:
      clusterName: development
      management:
        autoRepair: false
        replace:
          rollingUpdate:
            maxSurge: 1
            maxUnavailable: 0
          strategy: RollingUpdate
        upgradeType: Replace
      platform:
        aws:
          instanceType: m5.large
        type: AWS
      release:
        image: quay.io/openshift-release-dev/ocp-release:4.11.8-x86_64
      replicas: 2
  infrastructure:
    cloudProvider:
      name: aws-cred
    configure: True
    platform:
      aws:
        region: us-west-2
----

* Deploy the cluster
+
[source,sh]
----
oc apply -f development.yaml
----

* Wait until the cluster shows deployed:
+
[source,sh]
----
watch -n 10 oc get hypershiftdeployment -n aws-hypershift
----
+
.Sample Output
[source]
----
NAME	  TYPE   INFRA                  IAM                    MANIFESTWORK           PROVIDER REF   PROGRESS    AVAILABLE
development    AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Completed   True
awsprod   AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Partial     True
----

== Access cluster(s)

The kubeadmin password and kubeconfig file are stored in secrets in the local-cluster namespace.

* `<clustername>-kubeadmin-password`
* `<clustername>-admin-kubeconfig`

Get the kubeadmin password:

[source,sh]
----
oc get secret development-kubeadmin-password -n local-cluster --template='{{ .data.password }}' | base64 -d ; echo
----

Get the kubeconfig file and save it as `$HOME/kubeconfig-<clustername>.yaml`
[source,sh]
----
oc get secret development-admin-kubeconfig -n local-cluster --template='{{ .data.kubeconfig }}' | base64 -d >$HOME/kubeconfig-development.yaml
----

Set the KUBECONFIG variable to point to the new kube config file
[source,sh]
----
export KUBECONFIG=$HOME/kubeconfig-development.yaml
----

Validate the configuration
[source,sh]
----
oc get co
----

Get the console URL
[source,sh]
----
oc whoami --show-console
----

Log into the console using `kubeadmin` and the previously retrieved kubeadmin password.

Unset the KUBECONFIG variable to work back on your local cluster.
[source,sh]
----
unset KUBECONFIG
----
