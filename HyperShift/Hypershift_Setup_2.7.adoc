= HyperShift Setup for ACM 2.7

== Overview

This document describes how to install and configure Hypershift from scratch. It also explains how to deploy a managed cluster using Hypershift.

This document is not part of the lab (the environment has been pre-provisioned) but rather is provided as background information to explain what has been set up.

== Docs

* RHACM 2.5: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html/clusters/managing-your-clusters#hosted-control-plane-intro
* RHACM 2.6: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/multicluster_engine/multicluster_engine_overview#hosted-control-planes-intro
* RHACM 2.7: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.7/html/multicluster_engine/multicluster_engine_overview#hosted-control-planes-intro


== Prerequisites

. Deploy an OpenShift 4.12 (RC) cluster
. On that cluster install Advanced Cluster Manager 2.7 (operator and operand)
* For beta/upstream code deploy as outlined in https://github.com/stolostron/deploy
. Get your AWS credentials ready
* AWS Access Key
* AWS Secret Access Key
* Top level domain (sandboxXXXX.opentlc.com)
. Retrieve your privae and public key (e.g. ~/.ssh/${GUID}key.pem and ~/.ssh/${GUID}key.pub)
. Make sure you have your OpenShift Pull Secret in a file `$HOME/pullsecret.json`

=== Set up the Hypershift CLI

. Install make
+
[source,sh]
----
sudo dnf -y install make
----

. Download and install the `yq` binary
+
[source,sh]
----
sudo wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /usr/bin/yq
sudo chmod +x /usr/bin/yq
----

. Install go
+
[source,sh]
----
wget https://go.dev/dl/go1.20.1.linux-amd64.tar.gz
sudo tar -C /usr/local -xzf go1.20.1.linux-amd64.tar.gz
rm go1.20.1.linux-amd64.tar.gz
echo "export PATH=$PATH:/usr/local/go/bin" >>~/.bashrc
source ~/.bashrc
----

. Download, build and install the Hypershift CLI
+
[source,sh]
----
git clone https://github.com/openshift/hypershift.git
cd hypershift
make hypershift
sudo install -m 0755 bin/hypershift /usr/bin/hypershift
cd $HOME
----

=== Deploy Open Cluster Management

. Clone the repo
+
[source,sh]
----
git clone https://github.com/stolostron/deploy.git
----

. Generate your `~/deploy/prereqs/pull-secret.yaml` as per https://github.com/stolostron/deploy#prepare-to-deploy-open-cluster-management-instance-only-do-once

. Install Open Cluster Management
+
[source,sh]
----
cd ~/deploy
./start.sh --watch --search
----

. Use a recent 2.7 Snapshot from https://quay.io/repository/stolostron/acm-custom-registry?tab=tags (e.g. `2.7.0-SNAPSHOT-2023-01-18-23-04-43`) when prompted.

. Wait for the install to finish.

=== Set up AWS

. Set up AWS Credentials in file `~/.aws/credentials`
+
[source,texinfo]
----
[default]
aws_access_key_id = YOUR_AWS_ACCESS_KEY_ID
aws_secret_access_key = YOUR_AWS_SECRET_ACCESS_KEY
----

=== Setup Hypershift

. Hypershift needs an AWS S3 Bucket to store OICD documents (although Minio or OpenShift Data Foundations should work as well).
+
. Create an AWS S3 Bucket:
+
[source,sh]
----
aws s3api create-bucket \
  --bucket oidc-storage-${GUID} \
  --region us-east-2 \
  --create-bucket-configuration LocationConstraint=us-east-2
----
. Create a secret with AWS credential information in the `local-cluster` namespace:
+
[source,sh]
----
oc create secret generic hypershift-operator-oidc-provider-s3-credentials \
  -n local-cluster \
  --from-file=credentials=$HOME/.aws/credentials \
  --from-literal=bucket=oidc-storage-${GUID} \
  --from-literal=region=us-east-2

oc label secret hypershift-operator-oidc-provider-s3-credentials \
  -n local-cluster \
  cluster.open-cluster-management.io/backup=true
----

. Enable the HyperShift Preview Tech Preview:
+
[source,sh]
----
oc patch mce multiclusterengine \
  -n multicluster-engine \
  --type=merge \
  --patch '{"spec":{"overrides":{"components":[{"name":"hypershift-preview","enabled": true}]}}}'
----

. Wait for the Hypershift addon to be installed:
+
[source,sh]
----
oc wait --for=condition=Degraded=True managedclusteraddons/hypershift-addon -n local-cluster --timeout=5m
oc wait --for=condition=Available=True managedclusteraddons/hypershift-addon -n local-cluster --timeout=5m
----

. Validate that the addon is available:
+
[source,sh]
----
oc get managedclusteraddon hypershift-addon -n local-cluster
----
+
.Sample Output
[source,texinfo]
----
NAME               AVAILABLE   DEGRADED   PROGRESSING
hypershift-addon   True        False
----

Your Red Hat Advanced Cluster Management for Kubernetes is now configured for the Hypershift Tech Preview.

== Deploy a Hosted Cluster using the hypershift CLI (All in one)

Using the `hypershift` CLI you can now deploy a hosted cluster.

. Setup environment variables to deploy a cluster *development-${GUID}* in AWS_region us-west-2.
+
[source,sh]
----
# OpenShift specific variables
export OCP_CLUSTER_NAME=development-${GUID}
export OCP_RELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.0-x86_64
export OCP_INFRA_ID=development-${GUID}
export OCP_BASE_DOMAIN=sandbox948.opentlc.com
export OCP_PULL_SECRET=${HOME}/pullsecret.json

# AWS specific variables
export AWS_REGION=us-west-2
export AWS_BUCKET_NAME=oidc-storage-${GUID}
export AWS_BUCKET_REGION=us-east-2
export AWS_CREDS_FILE=${HOME}/.aws/credentials
----

. Create the hosted cluster:
+
[source,sh]
----
# With documentation - this does not copy/paste
# Use examples below
hypershift create cluster aws \
    --name ${OCP_CLUSTER_NAME} \
    --infra-id ${OCP_INFRA_ID} \
    --pull-secret ${OCP_PULL_SECRET} \
    --aws-creds ${AWS_CREDS_FILE} \
    --region ${AWS_REGION} \
    # Single zone setup, can add more zones \
    --zones ${AWS_REGION}a \
     # Default is m5.large which is usually too small \
    --instance-type m6a.2xlarge \
    --root-volume-type gp3 \
    --root-volume-size 120 \
    --base-domain ${OCP_BASE_DOMAIN} \
    --generate-ssh \
    # SingleReplica or HighlyAvailable (needs 3 worker nodes)
    --control-plane-availability-policy SingleReplica \
    # Calico, OVNKubernetes, OpenShiftSDN \
    --network-type OVNKubernetes \
    # optional, if omitted same as hub cluster
    --release-image ${OCP_RELEASE_IMAGE}
    --node-pool-replicas 2 \
    --namespace local-cluster

# Example
hypershift create cluster aws \
    --name ${OCP_CLUSTER_NAME} \
    --infra-id ${OCP_INFRA_ID} \
    --pull-secret ${OCP_PULL_SECRET} \
    --aws-creds ${AWS_CREDS_FILE} \
    --region ${AWS_REGION} \
    --zones ${AWS_REGION}a \
    --instance-type m6a.2xlarge \
    --root-volume-type gp3 \
    --root-volume-size 120 \
    --base-domain ${OCP_BASE_DOMAIN} \
    --generate-ssh \
    --control-plane-availability-policy SingleReplica \
    --network-type OVNKubernetes \
    --release-image ${OCP_RELEASE_IMAGE}
    --node-pool-replicas 2 \
    --namespace clusters

# Production Cluster
export OCP_CLUSTER_NAME=production-${GUID}
export OCP_INFRA_ID=production-${GUID}
hypershift create cluster aws \
    --name ${OCP_CLUSTER_NAME} \
    --infra-id ${OCP_INFRA_ID} \
    --pull-secret ${OCP_PULL_SECRET} \
    --aws-creds ${AWS_CREDS_FILE} \
    --region ${AWS_REGION} \
    --zones ${AWS_REGION}a \
    --instance-type m6a.2xlarge \
    --root-volume-type gp3 \
    --root-volume-size 120 \
    --base-domain ${OCP_BASE_DOMAIN} \
    --generate-ssh \
    --control-plane-availability-policy SingleReplica \
    --network-type OVNKubernetes \
    --release-image ${OCP_RELEASE_IMAGE}
    --node-pool-replicas 2 \
    --namespace clusters
----

. Wait until hosted cluster is available
+
[source,sh]
----
oc get hostedclusters -n ${OCP_CLUSTER_NAME}
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                VERSION   KUBECONFIG                           PROGRESS   AVAILABLE   PROGRESSING   MESSAGE
development-wk412             development-wk412-admin-kubeconfig   Partial    True        False         The hosted control plane is available
----

. Check MachineSets for hosted cluster (and wait until all replicas are ready and available):
+
[source,sh]
----
oc get machineset.cluster -A
----
+
.Sample Output
[source,texinfo]
----
NAMESPACE                         NAME                                      CLUSTER             REPLICAS   READY   AVAILABLE   AGE     VERSION
local-cluster-development-wk412   development-wk412-us-west-2a-5454cdd59b   development-wk412   2                              3m54s   4.12.0
----

. You can also check the Machines:
+
[source,sh]
----
oc get machine.cluster -A
----
+
.Sample Output
[source,texinfo]
----
NAMESPACE                         NAME                                            CLUSTER             NODENAME   PROVIDERID   PHASE          AGE     VERSION
local-cluster-development-wk412   development-wk412-us-west-2a-5454cdd59b-6xz9t   development-wk412                           Provisioning   4m43s   4.12.0
local-cluster-development-wk412   development-wk412-us-west-2a-5454cdd59b-zdpk4   development-wk412                           Provisioning   4m43s   4.12.0
----

== Deploy a Hosted Cluster using the hypershift CLI (individual steps)

Using the `hypershift` CLI you can now deploy a hosted cluster.

. Set some environment variables to deploy a cluster *development* in AWS_region us-west-2.
+
[source,sh]
----
# OpenShift specific variables
export OCP_CLUSTER_NAME=development-${GUID}
export OCP_RELEASE_IMAGE=quay.io/openshift-release-dev/ocp-release:4.12.0-x86_64
export OCP_INFRA_ID=${OCP_CLUSTER_NAME}
export OCP_BASE_DOMAIN=sandbox948.opentlc.com
export OCP_PULL_SECRET=${HOME}/pullsecret.json

# AWS specific variables
export AWS_REGION=us-west-2
export AWS_BUCKET_NAME=oidc-storage-${GUID}
export AWS_BUCKET_REGION=us-east-2
export AWS_CREDS_FILE=${HOME}/.aws/credentials
export AWS_INFRA_OUTPUT_FILE=${HOME}/aws-output.json
export AWS_IAM_OUTPUT_FILE=${HOME}/aws-iam-output.json
----

. Create the AWS infrastructure resources for the cluster:
+
[source,sh]
----
hypershift create infra aws \
  --aws-creds ${AWS_CREDS_FILE} \
  --base-domain ${OCP_BASE_DOMAIN} \
  --infra-id ${OCP_INFRA_ID} \
  --name ${OCP_CLUSTER_NAME} \
  --region ${AWS_REGION} \
  --output-file ${AWS_INFRA_OUTPUT_FILE}
----

. Retrieve information from AWS outputfile and save as variables:
+
[source,sh]
----
export AWS_MACHINE_CIDR=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.machineCIDR' | tr -d '"')
export AWS_VPC_ID=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.vpcID' | tr -d '"')
export AWS_ZONE_NAME=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.zones[0] .name' | tr -d '"')
export AWS_ZONE_SUBNET_ID=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.zones[0] .subnetID' | tr -d '"')
export AWS_SECURITY_GROUP_ID=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.securityGroupID' | tr -d '"')
export AWS_PUBLIC_ZONE_ID=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.publicZoneID' | tr -d '"')
export AWS_PRIVATE_ZONE_ID=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.privateZoneID' | tr -d '"')
export AWS_LOCAL_ZONE_ID=$(cat ${AWS_INFRA_OUTPUT_FILE} | jq '.localZoneID' | tr -d '"')
----

. Create AWS IAM resources:
+
[source,sh]
----
hypershift create iam aws \
  --aws-creds ${AWS_CREDS_FILE} \
  --infra-id ${OCP_INFRA_ID} \
  --local-zone-id ${AWS_LOCAL_ZONE_ID} \
  --private-zone-id ${AWS_PRIVATE_ZONE_ID} \
  --public-zone-id ${AWS_PUBLIC_ZONE_ID} \
  --oidc-storage-provider-s3-bucket-name ${AWS_BUCKET_NAME} \
  --oidc-storage-provider-s3-region ${AWS_BUCKET_REGION} \
  --output-file ${AWS_IAM_OUTPUT_FILE}
----

. Retrieve information from AWS IAM outputfile and save as variables:
+
[source,sh]
----
export AWS_IAM_PROFILE_NAME=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.profileName' | tr -d '"')
export AWS_IAM_ISSUER_URL=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.issuerURL' | tr -d '"')
export AWS_IAM_ROLES_INGRESS_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .ingressARN' | tr -d '"')
export AWS_IAM_ROLES_IMG_REGISTRY_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .imageRegistryARN' | tr -d '"')
export AWS_IAM_ROLES_STORAGE_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .storageARN' | tr -d '"')
export AWS_IAM_ROLES_NETWORK_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .networkARN' | tr -d '"')
export AWS_IAM_ROLES_KUBE_CLOUD_CONTROLLER_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .kubeCloudControllerARN' | tr -d '"')
export AWS_IAM_ROLES_NODEPOOL_MGMT_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .nodePoolManagementARN' | tr -d '"')
export AWS_IAM_ROLES_CPO_ARN=$(cat ${AWS_IAM_OUTPUT_FILE} | jq '.roles .controlPlaneOperatorARN' | tr -d '"')
----

. Either create the hosted cluster via CLI:
+
[source,sh]
----
hypershift create cluster aws \
  --name ${OCP_CLUSTER_NAME} \
  --infra-id ${OCP_INFRA_ID} \
  --infra-json ${AWS_INFRA_OUTPUT_FILE} \
  --iam-json ${AWS_IAM_OUTPUT_FILE} \
  --aws-creds ${AWS_CREDS_FILE} \
  --pull-secret ${OCP_PULL_SECRET} \
  --region ${AWS_REGION} \
  --control-plane-availability-policy SingleReplica \
  --network-type OVNKubernetes \
  --generate-ssh \
  --node-pool-replicas 3 \
  --instance-type m6a.2xlarge \
  --root-volume-type gp3 \
  --root-volume-size 120 \
  --namespace local-cluster

oc annotate hostedcluster development-${GUID} \
  -n local-cluster \
  cluster.open-cluster-management.io/hypershiftdeployment=local-cluster/${OCP_CLUSTER_NAME}

oc annotate hostedcluster development-${GUID} \
  -n local-cluster \
  cluster.open-cluster-management.io/managedcluster-name=${OCP_CLUSTER_NAME}
----

. Or create the hosted cluster via YAML:
.. Create secrets for pull secret, SSH Key and etcd encryption key:
+
[source,sh]
----
# Pull Secret
oc create secret generic -n local-cluster \
   ${OCP_CLUSTER_NAME}-pull-secret \
   --from-file=.dockerconfigjson=${OCP_PULL_SECRET}

# SSH Key for worker nodes
oc create secret generic -n local-cluster \
  ${OCP_CLUSTER_NAME}-ssh-key \
  --from-file=id_rsa=${HOME}/.ssh/id_rsa \
  --from-file=id_rsa.pub=${HOME}/.ssh/id_rsa.pub

# Random ETCD encryption key
oc create secret generic -n local-cluster \
  ${OCP_CLUSTER_NAME}-etcd-encryption-key \
  --from-literal=key=$(tr -dc A-Za-z0-9 </dev/urandom | head -c 32)
----

.. Create htpasswd file:
+
[source,sh]
----
cat <<EOF >${HOME}/${OCP_CLUSTER_NAME}-htpasswd.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: htpasswd-secret
  namespace: local-cluster
  labels:
    api.openshift.com/id: ${OCP_INFRA_ID}
    api.openshift.com/name: ${OCP_CLUSTER_NAME}
    api.openshift.com/type: identity-provider
type: Opaque
data:
  htpasswd: YW5kcmV3OiRhcHIxJGRaUGIyRUNmJGVyY2V2T0ZPNXpucnluVWZVajR0Yi8Ka2FybGE6JGFwcjEkRlF4Mm1YNGMkZUpjMjFHdVZaV05nMVVMRjhJMkczMQphZG1pbjokYXByMSRGOFMuVVZaciRPdmNhTW54UTladExFdHNBam1manIuCnVzZXIxOiRhcHIxJGdnQ2FpbWVZJE9KZEZob1NmTk91VzB1N2swRE4zby4KdXNlcjI6JGFwcjEkZ2dDYWltZVkkT0pkRmhvU2ZOT3VXMHU3azBETjNvLgp1c2VyMzokYXByMSRnZ0NhaW1lWSRPSmRGaG9TZk5PdVcwdTdrMEROM28uCnVzZXI0OiRhcHIxJGdnQ2FpbWVZJE9KZEZob1NmTk91VzB1N2swRE4zby4KdXNlcjU6JGFwcjEkZ2dDYWltZVkkT0pkRmhvU2ZOT3VXMHU3azBETjNvLgo=
EOF
----

.. Create the secret for htpasswd auth:
+
[source,sh]
----
oc apply -f ${HOME}/${OCP_CLUSTER_NAME}-htpasswd.yaml
----

.. Create hosted cluster:
+
[source,sh]
----
cat <<EOF >${HOME}/${OCP_CLUSTER_NAME}.yaml
---
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: ${OCP_CLUSTER_NAME}
  namespace: local-cluster
  annotations:
    cluster.open-cluster-management.io/hypershiftdeployment: local-cluster/${OCP_CLUSTER_NAME}
    cluster.open-cluster-management.io/managedcluster-name: ${OCP_CLUSTER_NAME}
spec:
  autoscaling: {}
  configuration:
    oauth:
      identityProviders:
      - name: htpasswd
        type: HTPasswd
        mappingMethod: claim
        htpasswd:
          fileData:
            name: htpasswd-secret
  controllerAvailabilityPolicy: SingleReplica
  dns:
    baseDomain: ${OCP_BASE_DOMAIN}
    privateZoneID: ${AWS_PRIVATE_ZONE_ID}
    publicZoneID: ${AWS_PUBLIC_ZONE_ID}
  etcd:
    managed:
      storage:
        persistentVolume:
          size: 4Gi
          storageClassName: gp3-csi
        type: PersistentVolume
    managementType: Managed
  fips: false
  infraID: ${OCP_INFRA_ID}
  infrastructureAvailabilityPolicy: SingleReplica
  issuerURL: https://${AWS_BUCKET_NAME}.s3.${AWS_BUCKET_REGION}.amazonaws.com/${OCP_CLUSTER_NAME}
  networking:
    clusterNetwork:
    - cidr: 10.132.0.0/14
    machineNetwork:
    - cidr: ${AWS_MACHINE_CIDR}
    networkType: OVNKubernetes
    serviceNetwork:
    - cidr: 172.31.0.0/16
  olmCatalogPlacement: management
  platform:
    aws:
      cloudProviderConfig:
        subnet:
          id: ${AWS_ZONE_SUBNET_ID}
        vpc: ${AWS_VPC_ID}
        zone: ${AWS_ZONE_NAME}
      endpointAccess: Public
      region: ${AWS_REGION}
      resourceTags:
      - key: kubernetes.io/cluster/${OCP_CLUSTER_NAME}
        value: owned
      rolesRef:
        controlPlaneOperatorARN: ${AWS_IAM_ROLES_CPO_ARN}
        imageRegistryARN: ${AWS_IAM_ROLES_IMG_REGISTRY_ARN}
        ingressARN: ${AWS_IAM_ROLES_INGRESS_ARN}
        kubeCloudControllerARN: ${AWS_IAM_ROLES_KUBE_CLOUD_CONTROLLER_ARN}
        networkARN: ${AWS_IAM_ROLES_NETWORK_ARN}
        nodePoolManagementARN: ${AWS_IAM_ROLES_NODEPOOL_MGMT_ARN}
        storageARN: ${AWS_IAM_ROLES_STORAGE_ARN}
    type: AWS
  pullSecret:
    name: ${OCP_CLUSTER_NAME}-pull-secret
  release:
    image: ${OCP_RELEASE_IMAGE}
  secretEncryption:
    aescbc:
      activeKey:
        name:  ${OCP_CLUSTER_NAME}-etcd-encryption-key
    type: aescbc
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
  - service: OVNSbDb
    servicePublishingStrategy:
      type: Route
  sshKey:
    name: ${OCP_CLUSTER_NAME}-ssh-key
EOF
----

. Create the cluster:
+
[source,sh]
----
oc apply -f ${HOME}/${OCP_CLUSTER_NAME}.yaml
----

. Create a *NodePool* manifest:
+
[source,sh]
----
cat <<EOF >${HOME}/${OCP_CLUSTER_NAME}-node-pool.yaml
---
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: ${OCP_CLUSTER_NAME}-workers
  namespace: local-cluster
spec:
  clusterName: ${OCP_CLUSTER_NAME}
  management:
    autoRepair: true
    replace:
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 0
      strategy: RollingUpdate
    upgradeType: Replace
  platform:
    aws:
      instanceProfile: ${OCP_INFRA_ID}-worker
      instanceType: m6a.2xlarge
      rootVolume:
        size: 120
        type: gp3
      securityGroups:
      - id: ${AWS_SECURITY_GROUP_ID}
      subnet:
        id: ${AWS_ZONE_SUBNET_ID}
    type: AWS
  release:
    image: ${OCP_RELEASE_IMAGE}
  replicas: 2
EOF
----

. Create the NodePool:
+
[source,sh]
----
oc apply -f ${HOME}/${OCP_CLUSTER_NAME}-node-pool.yaml
----

=== Import the cluster into RHACM

. Create *Managed Cluster* resource:
+
[source,sh]
----
cat <<EOF >${HOME}/${OCP_CLUSTER_NAME}-managed-cluster.yaml
---
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  annotations:
    import.open-cluster-management.io/hosting-cluster-name: local-cluster
    import.open-cluster-management.io/klusterlet-deploy-mode: Hosted
    open-cluster-management/created-via: other
  labels:
    cloud: auto-detect
    cluster.open-cluster-management.io/clusterset: default
    name: ${OCP_CLUSTER_NAME}
    vendor: OpenShift
    type: sandbox
    purpose: development
  name: ${OCP_CLUSTER_NAME}
spec:
  hubAcceptsClient: true
  leaseDurationSeconds: 60
EOF
----

. Apply the `ManagedCluster` resource:
+
[source,sh]
----
oc apply -f ${HOME}/${OCP_CLUSTER_NAME}-managed-cluster.yaml
----

. Create the `KlusterletAddonConfig` for the managed cluster to deploy all the management agents:
+
[source,sh]
----
cat <<EOF >${HOME}/${OCP_CLUSTER_NAME}-klusterletaddonconfig.yaml
---
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: ${OCP_CLUSTER_NAME}
  namespace: ${OCP_CLUSTER_NAME}
spec:
  clusterName: ${OCP_CLUSTER_NAME}
  clusterNamespace: ${OCP_CLUSTER_NAME}
  clusterLabels:
    cloud: auto-detect
    vendor: auto-detect
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: false
EOF
----

. Apply the `KlusterletAddonConfig` resource:
+
[source,sh]
----
oc apply -f ${HOME}/${OCP_CLUSTER_NAME}-klusterletaddonconfig.yaml
----

That's it. Your hosted cluster is now deployed and imported into ACM.

== Destroy Hosted Clusters manually in case of failure

. Destroy hosted cluster via CLI:
+
[source,sh]
----
hypershift destroy cluster aws \
  --name ${OCP_CLUSTER_NAME} \
  --infra-id ${OCP_INFRA_ID} \
  --aws-creds ${AWS_CREDS_FILE} \
  --namespace local-cluster
----

. Destroy AWS IAM resources:
+
[source,sh]
----
hypershift destroy iam aws --aws-creds ${AWS_CREDS_FILE} --infra-id ${OCP_INFRA_ID}
----

// . Empty and delete OIDC bucket:
// +
// [source,sh]
// ----
// aws s3 rm s3://${AWS_BUCKET_NAME} --recursive
// aws s3 rb s3://${AWS_BUCKET_NAME}
// oc delete secret hypershift-operator-oidc-provider-s3-credentials -n local-cluster
// ----

. Destroy AWS infrastructure resources:
+
[source,sh]
----
hypershift destroy infra aws \
  --aws-creds ${AWS_CREDS_FILE} \
  --base-domain ${OCP_BASE_DOMAIN} \
  --infra-id ${OCP_INFRA_ID} \
  --region ${AWS_REGION}
----

== Access cluster(s)

The kubeadmin password and kubeconfig file are stored in secrets in the `local-cluster`namespace.

* `<clustername>-kubeadmin-password`
* `<clustername>-admin-kubeconfig`

. Get the kubeadmin password (only works if no authentication has been set):
+
[source,sh]
----
oc get secret ${OCP_CLUSTER_NAME}-kubeadmin-password -n local-cluster --template='{{ .data.password }}' | base64 -d ; echo
----

. Get the kubeconfig file and save it as `$HOME/kubeconfig-<clustername>.yaml`
+
[source,sh]
----
oc get secret ${OCP_CLUSTER_NAME}-admin-kubeconfig -n local-cluster --template='{{ .data.kubeconfig }}' | base64 -d >$HOME/kubeconfig-${OCP_CLUSTER_NAME}.yaml
----

. Set the KUBECONFIG variable to point to the new kube config file
+
[source,sh]
----
export KUBECONFIG=$HOME/kubeconfig-${OCP_CLUSTER_NAME}.yaml
----

. Validate the configuration
+
[source,sh]
----
oc get co
----

. Get the console URL
+
[source,sh]
----
oc whoami --show-console
----

. Log into the console using `kubeadmin` and the previously retrieved kubeadmin password.

. Unset the KUBECONFIG variable to work back on your local cluster.
+
[source,sh]
----
unset KUBECONFIG
----
