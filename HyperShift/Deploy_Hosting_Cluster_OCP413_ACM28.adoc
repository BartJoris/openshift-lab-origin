
= Instructions to deploy a Hosting (Hub) Cluster using RHACM 2.8 on OCP 4.13

:numbered:
:toc:

These instructions are to deploy and configure a hub cluster for RHPD PoC use. This cluster ban be used as a target cluster for new catalog items in RHPD which deploy OpenShift clusters using Hosted Control Planes.

== Deploy Hub Cluster

Use an AWS account that is outside of a Sandbox - Hypershift will need EIPs, Load Balancers, NAT Gateways etc and would run into capacity restrictions really quickly in a Sandbox account.

[NOTE]
----
The current PoC cluster is deployed in the `development` AWS account and uses the base domain `hyper.dev.redhatworkshops.io`.
----

Use the following vars file to deploy a new OpenShift 4.13 cluster. These vars also preconfigure the policies for RHDP clusters with hosted control planes:

.OpenShift 4.13 Cluster Vars
[source,yaml]
----
# -------------------------------------------------------------------
# Mandatory Variables
# -------------------------------------------------------------------
cloud_provider: ec2
env_type: ocp4-cluster
software_to_deploy: openshift4
# -------------------------------------------------------------------
# End Mandatory Variables
# -------------------------------------------------------------------

platform: labs
purpose: production

agnosticd_aws_capacity_reservation_enable: false
aws_region: us-east-2

ssh_authorized_keys:
- key: https://github.com/wkulhanek.keys

master_instance_type: m5a.2xlarge
master_instance_count: 3
master_storage_type: io1
master_storage_size: 250
worker_instance_type: m5a.4xlarge
worker_instance_count: 3
worker_storage_type: gp3
worker_storage_size: 250
bastion_instance_type: t3a.medium
bastion_instance_image: RHEL91GOLD-latest

install_ftl: false
install_student_user: false

ocp4_installer_version: "4.13"
ocp4_installer_root_url: http://mirror.openshift.com/pub/openshift-v4/clients

ocp4_network_type: OVNKubernetes

repo_method: satellite
update_packages: true
run_smoke_tests: false

# Change requirements.yaml from ocp4-cluster to include community.crypto
requirements_content:
  collections:
  - name: kubernetes.core
    version: 2.4.0
  - name: amazon.aws
    version: 6.1.0
  - name: community.general
    version: 7.1.0
  - name: ansible.posix
    version: 1.5.4
  - name: community.crypto
    version: 2.14.0

cloud_tags:
- owner: wkulhane@redhat.com
- Purpose: production
- env_type: "{{ env_type }}"
- guid: "{{ guid }}"

infra_workloads:
- ocp4_workload_machinesets
- ocp4_workload_le_certificates
- ocp4_workload_authentication
- ocp4_workload_pipelines
- ocp4_workload_openshift_gitops
- ocp4_workload_rhacm
- ocp4_workload_rhacm_hypershift

# Do not run any student customization.
student_workloads: []

# -------------------------------------------------------------------
# Workload variables
# -------------------------------------------------------------------

# -------------------------------------------------------------------
# ocp4_workload_machinesets
# -------------------------------------------------------------------

# Replace default MachineSets
ocp4_workload_machinesets_disable_default_machinesets: true

# Add MachineSets for autoscaled worker instances
ocp4_workload_machinesets_machineset_groups:
- name: autoscaled
  role: autoscaled
  instance_type: m5a.4xlarge
  root_volume_size: 250
  root_volume_type: gp3
  autoscale: true
  total_replicas: 3
  # Autoscaler settings - minimum and maximum number of Machines
  total_replicas_min: 3
  total_replicas_max: 15

# -------------------------------------------------------------------
# Workload: ocp4_workload_authentication
# -------------------------------------------------------------------
ocp4_workload_authentication_idm_type: htpasswd
ocp4_workload_authentication_admin_user: admin
# ocp4_workload_authentication_htpasswd_admin_password: ""
ocp4_workload_authentication_htpasswd_user_base: user
# ocp4_workload_authentication_htpasswd_user_password: ""
ocp4_workload_authentication_htpasswd_user_count: 5
ocp4_workload_authentication_remove_kubeadmin: true

# ---------------------------------------------------------
# OpenShift Pipelines
# ---------------------------------------------------------
ocp4_workload_pipelines_channel: pipelines-1.11
ocp4_workload_pipelines_use_catalog_snapshot: false

# ---------------------------------------------------------
# OpenShift Gitops
# ---------------------------------------------------------
ocp4_workload_openshift_gitops_channel: gitops-1.9

# -------------------------------------------------------------------
# Workload: ocp4_workload_rhacm
# -------------------------------------------------------------------
ocp4_workload_rhacm_acm_channel: release-2.8
ocp4_workload_rhacm_use_catalog_snapshot: false

# -------------------------------------------------------------------
# Workload: ocp4_workload_rhacm_hypershift
# -------------------------------------------------------------------
ocp4_workload_rhacm_hypershift_rhdp_policies_setup: true
ocp4_workload_rhacm_hypershift_managed_cluster_set_setup: true
ocp4_workload_rhacm_hypershift_deploy_clusters: []
----

== Set up `opentlc-mgr` user

RHDP uses the `opentlc-mgr` user on the bastion VM to deploy things. Therefore this user needs to be created and configured on the bastion VM.

. Switch to root:
+
[source,sh]
----
sudo -i
----

. Add the `opentlc-mgr` user:
+
[source,sh]
----
adduser opentlc-mgr
----

. Set up `.kube/config` to allow `opentlc-mgr` to work as `system:admin` on the cluster.
+
[source,sh]
----
cp -R /home/ec2-user/.kube /home/opentlc-mgr
chown -R opentlc-mgr:users /home/opentlc-mgr/.kube
----

. Set up SSH configuration for `opentlc-mgr`:
+
[source,sh]
----
mkdir /home/opentlc-mgr/.ssh
----

. Add the `opentlc-mgr` *public SSH key* to be used from RHPD to the `authorized_keys` file.
+
[source,sh]
----
cat <<EOF >/home/opentlc-mgr/.ssh/authorized_keys
# OpenTLC Admin Backdoor
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCvZvn+GL0wTOsAdh1ikIQoqj2Fw/RA6F14O347rgKdpkgOQpGQk1k2gM8wcla2Y1o0bPIzwlNy1oh5o9uNjZDMeDcEXWuXbu0cRBy4pVRhh8a8zAZfssnqoXHHLyPyHWpdTmgIhr0UIGYrzHrnySAnUcDp3gJuE46UEBtrlyv94cVvZf+EZUTaZ+2KjTRLoNryCn7vKoGHQBooYg1DeHLcLSRWEADUo+bP0y64+X/XTMZOAXbf8kTXocqAgfl/usbYdfLOgwU6zWuj8vxzAKuMEXS1AJSp5aeqRKlbbw40IkTmLoQIgJdb2Zt98BH/xHDe9xxhscUCfWeS37XLp75J

# AgnosticD Config opentlc/SHARED_OCP412_HYPERSHIFT_CLUSTER
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDm/eeXtVCndZiIcOK3DZUTYAU5a1hbkakM99x6HRNmeRJhoPUEP9jcVAdQdLmvOvNaZHQwonDl2xxXCP0FaOnNw8ARL9z8Y4s9+QZ/yf8V7fHgy3EVxXZOMslENVMiZFch1M9bnoQVe7e91+MfZR26mxLJqydjez2R1Hx3u85WIZFzKo7v2XqB3yXuGMRwdwsZI9zFq9CUSexAW43ctDKyt6v1xQPhpJ3RjJOCo0aGOpQhP0/vlOoeAYgm9+C2oeSBmXGNd44SsU0TfiZuRvLUJvOP8Kd8kwwExzgW4K7Oo+PF9hinivaUxE2tG246UHpgjH6XOuSjl/l68PP3cv0F
EOF
----

. Change permissions for the directory and file:
+
[source,sh]
----
chown -R opentlc-mgr:users /home/opentlc-mgr/.ssh
chmod 0700 /home/opentlc-mgr/.ssh
chmod 0644 /home/opentlc-mgr/.ssh/*
----

. Exit the root shell
+
[source,sh]
----
exit
----

== Configure Machine Management (Autoscale / Health checks)

The configuration above sets up autoscaling for the worker nodes. But it does not (yet) set up `MachineHealthCheck` resources.

WKTBD: Set MS1/MS2/MS3 variables based on created machinesets

. Create Machine Health Checks for all three machine sets
+
[source,sh]
----
cat <<EOF >${HOME}/machinemanagement/mhc-${MS1}.yaml
---
apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: mhc-${MS1}
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: ${MS1}
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s"
    status: "False"
  - type:    "Ready"
    timeout: "300s"
    status: "Unknown"
  maxUnhealthy: "40%"
  nodeStartupTimeout: "10m"
EOF

cat <<EOF >${HOME}/machinemanagement/mhc-${MS2}.yaml
---
apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: mhc-${MS2}
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: ${MS2}
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s"
    status: "False"
  - type:    "Ready"
    timeout: "300s"
    status: "Unknown"
  maxUnhealthy: "40%"
  nodeStartupTimeout: "10m"
EOF

cat <<EOF >${HOME}/machinemanagement/mhc-${MS3}.yaml
---
apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: mhc-${MS3}
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: ${MS3}
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s"
    status: "False"
  - type:    "Ready"
    timeout: "300s"
    status: "Unknown"
  maxUnhealthy: "40%"
  nodeStartupTimeout: "10m"
EOF
----

. Apply all resources to the cluster:
+
[source,sh]
----
for resource in ${HOME}/machinemanagement/*.yaml; do; oc apply -f ${resource}; done
----

== Deploy clusters via Pipelines (future possible change)

. Install the Openshift Pipelines Operator (to be added to vars file)

TBD: https://github.com/jnpacker/hypershift-pipelines
